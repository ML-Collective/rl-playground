**Resources found used during implementation sessions:**

* [Link](https://www.youtube.com/watch?v=5P7I-xPq8u8&t=952s) to a video shared on the basics of PPO
* [Link](https://math.stackexchange.com/questions/3108216/change-of-variables-apply-tanh-to-the-gaussian-samples) explains the use of the TANH as the activation function on the Feed Forward
* [Implementation](https://github.com/nikhilbarhate99/PPO-PyTorch/blob/master/PPO_colab.ipynb) we referred to for how to code the multivariatenormal policy
* Paper:["Benchmarking Deep Reinforcement Learning for Continuous Control"](https://arxiv.org/pdf/1604.06778.pdf) useful for standards adopted
* Multiprocessing with SB3 - [Tutorial](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb#scrollTo=BIedd7Pz9sOs)
* SB3 - [SubprocVecEnv](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#subprocvecenv)
* Research Coding [best practices](https://goodresearch.dev/#the-good-research-code-handbook)
* Seeds and reproducibility [pytorch](https://pytorch.org/docs/stable/notes/randomness.html)
* Cleanrl's [blog](https://costa.sh/blog-the-32-implementation-details-of-ppo.html) on improving PPO
* GAE [blog](https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/) from Seita's place



